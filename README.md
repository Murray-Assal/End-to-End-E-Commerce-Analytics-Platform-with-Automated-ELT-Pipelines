# E-Commerce Data Platform

A production-ready, end-to-end data engineering pipeline built with modern data stack tools. This project demonstrates the complete data lifecycle from extraction to visualization, showcasing best practices in data engineering, ETL/ELT workflows, and analytics.

![Data Engineering](https://img.shields.io/badge/Data-Engineering-blue)
![Python](https://img.shields.io/badge/Python-3.12-green)
![Docker](https://img.shields.io/badge/Docker-Compose-blue)
![Airflow](https://img.shields.io/badge/Apache-Airflow_3.1.1-red)
![dbt](https://img.shields.io/badge/dbt-1.7-orange)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-16-blue)
![Metabase](https://img.shields.io/badge/Metabase-Latest-purple)

## Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Tech Stack](#tech-stack)
- [Features](#features)
- [Data Model](#data-model)
- [Getting Started](#getting-started)
- [Project Structure](#project-structure)
- [Dashboards](#dashboards)
- [Data Quality](#data-quality)
- [Lessons Learned](#lessons-learned)
- [Future Enhancements](#future-enhancements)
- [Contributing](#contributing)
- [License](#license)

## Overview

This project implements a **modern data warehouse** for an e-commerce business, providing actionable insights through automated data pipelines and interactive dashboards. The platform processes data from multiple sources, transforms it using industry-standard tools, and serves it to business users through intuitive visualizations.

**Key Capabilities:**

- **Real-time Analytics**: Daily automated data refreshes
- **Scalable ETL**: Modular, maintainable data pipelines
- **Business Intelligence**: 5 comprehensive dashboards covering sales, customers, and products
- **Data Quality**: Built-in validation and cleaning layers
- **Containerized**: Fully dockerized for easy deployment

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DATA SOURCES                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚  DummyJSON API   â”‚              â”‚    Stripe API       â”‚          â”‚
â”‚  â”‚  â€¢ Products      â”‚              â”‚    â€¢ Payments       â”‚          â”‚
â”‚  â”‚  â€¢ Users         â”‚              â”‚    â€¢ Refunds        â”‚          â”‚
â”‚  â”‚  â€¢ Carts         â”‚              â”‚    â€¢ Invoices       â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ORCHESTRATION (Apache Airflow)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  DAG: ecommerce_data_extraction                                â”‚ â”‚
â”‚  â”‚  â€¢ Schedule: @daily                                            â”‚ â”‚
â”‚  â”‚  â€¢ Extracts raw data via REST APIs                             â”‚ â”‚
â”‚  â”‚  â€¢ Parallel execution for independent sources                  â”‚ â”‚
â”‚  â”‚  â€¢ Error handling & retry logic                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA WAREHOUSE (PostgreSQL)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  RAW LAYER                                                   â”‚   â”‚
â”‚  â”‚  â€¢ raw_products, raw_users, raw_carts                        â”‚   â”‚
â”‚  â”‚  â€¢ raw_orders, raw_order_items                               â”‚   â”‚
â”‚  â”‚  â€¢ raw_stripe_payments, raw_stripe_refunds                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRANSFORMATION (dbt)                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  STAGING LAYER (Views)                                       â”‚   â”‚
â”‚  â”‚  â€¢ stg_products, stg_users, stg_orders                       â”‚   â”‚
â”‚  â”‚  â€¢ Data cleaning, type casting, JSONB extraction             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  INTERMEDIATE LAYER (Views)                                  â”‚   â”‚
â”‚  â”‚  â€¢ int_users_cleaned_locations (Data quality fixes)          â”‚   â”‚
â”‚  â”‚  â€¢ Business logic, data enrichment                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  MARTS LAYER (Tables)                                        â”‚   â”‚
â”‚  â”‚  Core:                                                       â”‚   â”‚
â”‚  â”‚    â€¢ dim_products, dim_customers                             â”‚   â”‚
â”‚  â”‚    â€¢ fact_orders, fact_order_items                           â”‚   â”‚
â”‚  â”‚  Finance:                                                    â”‚   â”‚
â”‚  â”‚    â€¢ daily_revenue, product_performance                      â”‚   â”‚
â”‚  â”‚    â€¢ customer_segmentation                                   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VISUALIZATION (Metabase)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  â€¢ Executive Overview Dashboard                              â”‚   â”‚
â”‚  â”‚  â€¢ Sales Performance Dashboard                               â”‚   â”‚
â”‚  â”‚  â€¢ Customer Analytics Dashboard                              â”‚   â”‚
â”‚  â”‚  â€¢ Product Performance Dashboard                             â”‚   â”‚
â”‚  â”‚  â€¢ Operations & KPIs Dashboard                               â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Tech Stack

### Orchestration

- **Apache Airflow 3.1.1**: Workflow orchestration with TaskFlow API
- **Docker Compose**: Container orchestration

### Storage

- **PostgreSQL 16**: Data warehouse
- **Separate databases**: Airflow metadata, Metabase metadata, data warehouse

### Transformation

- **dbt (Data Build Tool)**: SQL-based transformations
- **Python 3.12**: Custom data processing scripts

### Visualization

- **Metabase**: Self-service BI platform

### Data Sources

- **DummyJSON API**: Mock e-commerce data (products, users, carts)
- **Stripe API**: Payment processing data (optional)

## Features

### Data Pipeline

- **Automated Daily Extraction**: Airflow DAG runs daily at midnight
- **Parallel Processing**: Independent API calls run concurrently
- **Error Handling**: Retry logic with exponential backoff
- **Idempotent Loads**: TRUNCATE/INSERT pattern ensures data consistency
- **Incremental Ready**: Architecture supports incremental loads

### Data Transformation

- **Layered Architecture**: Raw â†’ Staging â†’ Intermediate â†’ Marts
- **Data Quality**: City/state validation with reference mapping
- **Type Safety**: Proper data type casting and validation
- **Documentation**: Auto-generated dbt docs
- **Testing**: Built-in data quality tests (uniqueness, not null)

### Analytics

- **Star Schema**: Optimized for analytical queries
- **Pre-aggregated Metrics**: Daily summaries for performance
- **Customer Segmentation**: RFM analysis for customer insights
- **Product Analytics**: Sales categories, inventory health
- **Financial Reporting**: Revenue trends, order metrics

## Data Model

### Dimensional Model (Star Schema)

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   dim_customers     â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ PK: user_id         â”‚
                    â”‚ â€¢ full_name         â”‚
                    â”‚ â€¢ email             â”‚
                    â”‚ â€¢ city_state        â”‚
                    â”‚ â€¢ age_group         â”‚
                    â”‚ â€¢ lifetime_value    â”‚
                    â”‚ â€¢ customer_segment  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â”‚
                             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   dim_products      â”‚  â”‚   fact_orders        â”‚  â”‚   fact_order_items  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PK: product_id      â”‚â†â”€â”‚ FK: product_id       â”‚  â”‚ FK: order_id        â”‚
â”‚ â€¢ product_name      â”‚  â”‚ FK: user_id          â”‚  â”‚ FK: product_id      â”‚
â”‚ â€¢ category          â”‚  â”‚ PK: order_id         â”‚â”€â†’â”‚ FK: user_id         â”‚
â”‚ â€¢ unit_price        â”‚  â”‚ â€¢ order_date         â”‚  â”‚ â€¢ quantity          â”‚
â”‚ â€¢ rating            â”‚  â”‚ â€¢ order_total        â”‚  â”‚ â€¢ line_total        â”‚
â”‚ â€¢ stock_quantity    â”‚  â”‚ â€¢ order_status       â”‚  â”‚ â€¢ discount_amount   â”‚
â”‚ â€¢ sales_category    â”‚  â”‚ â€¢ is_completed       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Metrics & Aggregations

**daily_revenue**

- Daily KPIs: revenue, orders, customers
- Average order value trends
- Order completion rates

**product_performance**

- Sales rankings by revenue/quantity
- Inventory health indicators
- Category performance metrics

**customer_segmentation**

- RFM analysis (Recency, Frequency, Monetary)
- Customer lifecycle stages
- Value-based segmentation

## Getting Started

### Prerequisites

- Docker & Docker Compose
- 4GB+ RAM
- 10GB+ disk space
- Python 3.12+ (for local dbt development)

### Installation

1. **Clone the repository**

   ```bash
   git clone https://github.com/Murray-Assal/End-to-End-E-Commerce-Analytics-Platform-with-Automated-ELT-Pipelines.git
   cd ecommerce-data-platform
   ```

2. **Set up environment variables**

   ```bash
   cp .env.example .env
   # Edit .env with your configurations
   ```

3. **Start the platform**

   ```bash
   docker-compose up -d
   ```

4. **Wait for services to initialize** (~2 minutes)

   ```bash
   docker-compose logs -f
   ```

5. **Access the services**

   - **Airflow**: <http://localhost:8080> (user: `airflow`, password: `airflow`)
   - **Metabase**: <http://localhost:3000>
   - **PostgreSQL**: `localhost:5433` (user: `admin`, password: `admin`)

### Initial Setup

1. **Trigger the extraction DAG**
   - Go to Airflow UI
   - Toggle on `ecommerce_data_extraction`
   - Click "Trigger DAG"

2. **Run dbt transformations**

   ```bash
   cd dbt_ecommerce
   pip install dbt-postgres
   cp profiles.yml ~/.dbt/profiles.yml
   dbt run
   ```

3. **Set up Metabase**
   - Go to <http://localhost:3000>
   - Create admin account
   - Connect to database:
     - Host: `postgres-data`
     - Port: `5432`
     - Database: `ecommerce_dw`
     - User: `admin`
     - Password: `admin`

4. **Build dashboards**
   - Use SQL queries from `METABASE_ANALYTICS_GUIDE.md`
   - Create the 5 pre-defined dashboards

## Project Structure

```
ecommerce-data-platform/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ecommerce_extraction_dag.py    # Airflow DAG for data extraction
â”œâ”€â”€ dbt_ecommerce/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â”œâ”€â”€ sources.yml            # Source definitions & tests
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_products.sql       # Staging: products
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_users.sql          # Staging: users
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_orders.sql         # Staging: orders
â”‚   â”‚   â”‚   â””â”€â”€ stg_order_items.sql    # Staging: order items
â”‚   â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”‚   â””â”€â”€ int_users_cleaned_locations.sql  # Data quality layer
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”‚       â”œâ”€â”€ core/
â”‚   â”‚       â”‚   â”œâ”€â”€ dim_products.sql   # Product dimension
â”‚   â”‚       â”‚   â”œâ”€â”€ dim_customers.sql  # Customer dimension
â”‚   â”‚       â”‚   â”œâ”€â”€ fact_orders.sql    # Order facts
â”‚   â”‚       â”‚   â””â”€â”€ fact_order_items.sql  # Order item facts
â”‚   â”‚       â””â”€â”€ finance/
â”‚   â”‚           â”œâ”€â”€ daily_revenue.sql  # Daily KPIs
â”‚   â”‚           â”œâ”€â”€ product_performance.sql
â”‚   â”‚           â””â”€â”€ customer_segmentation.sql
â”‚   â”œâ”€â”€ dbt_project.yml               # dbt configuration
â”‚   â”œâ”€â”€ profiles.yml                  # Database connection
â”‚   â””â”€â”€ README.md                     # dbt documentation
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ METABASE_SETUP_GUIDE.md      # Metabase setup instructions
â”‚   â”œâ”€â”€ METABASE_ANALYTICS_GUIDE.md  # Dashboard queries & tips
â”‚   â””â”€â”€ HOW_TO_VIEW_DATA.md          # Database access guide
â”œâ”€â”€ docker-compose.yml                # Container orchestration
â”œâ”€â”€ .env                              # Environment variables
â””â”€â”€ README.md                         # This file
```

## Dashboards

### 1. Executive Overview

**Purpose**: High-level KPIs for leadership

**Metrics**:

- Total Revenue, Total Orders, Average Order Value, Total Customers
- Revenue trend line chart
- Orders by status (pie chart)
- Top 5 products by revenue
- Customer segment distribution

### 2. Sales Performance

**Purpose**: Deep dive into sales metrics

**Metrics**:

- Revenue by product category
- Daily revenue trends
- Order size distribution
- Top 20 products table
- Discount effectiveness analysis

### 3. Customer Analytics

**Purpose**: Understand customer behavior

**Metrics**:

- Customer lifetime value distribution
- Top 10 customers by spend
- Demographics (age groups, gender, location)
- Customer segments (one-time, occasional, frequent)
- Repeat vs one-time customer ratio

### 4. Product Performance

**Purpose**: Analyze product catalog

**Metrics**:

- Products by category with ratings
- Best sellers table
- Low stock alerts
- Rating distribution
- Sales categories (bestseller, popular, regular)

### 5. Operations & KPIs

**Purpose**: Daily operations monitoring

**Metrics**:

- Today's performance (revenue, orders, customers, AOV)
- 7-day trend chart
- Order completion rate
- Recent orders table
- Average items per order

## Data Quality

### Built-in Validation

**Source Layer Tests** (in `sources.yml`):

- Primary key uniqueness
- Not null constraints on critical fields
- Referential integrity between tables

**Data Cleaning**:

- **City/State Correction**: Reference mapping for 50+ major US cities
- **Type Casting**: Proper numeric, date, and boolean types
- **JSONB Extraction**: Clean extraction from nested JSON structures
- **NULL Handling**: Coalesce patterns for missing values

**Monitoring**:

- Run `dbt test` after each transformation
- Check logs in `dbt_ecommerce/logs/dbt.log`
- Review data quality in Metabase dashboards

## Lessons Learned

### Technical Insights

1. **Airbyte Complexity**: Initially attempted Airbyte but pivoted to Python scripts due to:
   - Docker Compose deprecation in Airbyte 1.0+
   - Resource constraints (abctl requires 50-100GB)
   - Simpler architecture with direct Python â†’ PostgreSQL

2. **dbt Schema Configuration**: Learned about schema naming patterns:
   - `schema: public` + custom schema = `public_staging`, `public_marts`
   - Better to use separate schemas or configure custom naming

3. **Test Data Limitations**: DummyJSON has inherent data quality issues:
   - Implemented data quality layer to fix city/state mismatches
   - Good practice for real-world scenarios

4. **Container Orchestration**: Separation of concerns:
   - Airflow metadata â†’ `postgres` database
   - Metabase metadata â†’ `metabase` database  
   - Data warehouse â†’ `postgres-data` database
   - Prevents conflicts and enables independent scaling

### Best Practices Applied

- **Idempotent Pipelines**: TRUNCATE/INSERT pattern for full refreshes
- **Separation of Concerns**: Modular dbt models (staging â†’ marts)
- **Documentation as Code**: dbt docs and inline comments
- **Version Control**: All code in Git with meaningful commits
- **Testing**: Data quality tests at every layer
- **Monitoring**: Airflow task logs and dbt test results

## Future Enhancements

### Short Term

- [ ] Add incremental loading for large tables
- [ ] Implement SCD Type 2 for dim_customers
- [ ] Add data quality monitoring dashboard
- [ ] Set up email alerts for pipeline failures
- [ ] Add more dbt tests (relationships, accepted values)

### Medium Term

- [ ] Migrate to cloud (AWS/GCP/Azure)
- [ ] Add real-time streaming with Kafka
- [ ] Implement data lineage tracking
- [ ] Add machine learning models (customer churn prediction)
- [ ] Create Airflow sensor for external data availability

### Long Term

- [ ] Multi-tenant architecture
- [ ] Data catalog integration (DataHub/Amundsen)
- [ ] Implement data mesh principles
- [ ] Add cost monitoring and optimization
- [ ] Build self-service analytics platform

## Contributing

Contributions are welcome! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **DummyJSON**: For providing a reliable mock API for testing
- **Apache Airflow**: For robust workflow orchestration
- **dbt Labs**: For the incredible transformation framework
- **Metabase**: For making BI accessible and open-source
- **Data Engineering Community**: For best practices and inspiration

## Contact

Murad Asal - [My LinkedIn](https://www.linkedin.com/in/murad-asal-421ba7226) - <muradsaleh82@gmail.com>

Project Link: [https://github.com/Murray-Assal/End-to-End-E-Commerce-Analytics-Platform-with-Automated-ELT-Pipelines](https://github.com/Murray-Assal/End-to-End-E-Commerce-Analytics-Platform-with-Automated-ELT-Pipelines)

---

**If you found this project helpful, please consider giving it a star!**

---

## Project Statistics

- **Lines of SQL**: ~2,000
- **dbt Models**: 15+
- **Airflow Tasks**: 9
- **Data Tables**: 20+
- **API Endpoints**: 5
- **Dashboards**: 5
- **Metrics Tracked**: 50+

## Skills Demonstrated

- Data Pipeline Development
- ETL/ELT Architecture
- SQL & Data Modeling
- Python Programming
- Docker & Containerization
- Workflow Orchestration (Airflow)
- Data Transformation (dbt)
- Business Intelligence
- Data Quality Engineering
- Database Design
- API Integration
- Version Control (Git)
- Technical Documentation

---

Built with â¤ï¸ by Murad Asal | Last Updated: December 2025
